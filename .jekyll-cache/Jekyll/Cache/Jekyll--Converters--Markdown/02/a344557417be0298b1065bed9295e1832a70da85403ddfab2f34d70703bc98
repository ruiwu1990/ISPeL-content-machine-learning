I"±<ul>
  <li>What have we learned to evaluate a model so far?
    <ul>
      <li>Classification accuracy rate = correct/total</li>
      <li>Classification: confusion matrix</li>
      <li>Regression: RSS (usually used only for tuning coefficients)</li>
    </ul>
  </li>
  <li>What will be introduced:
    <ul>
      <li>Classification: ROC</li>
      <li>Regression: R^2</li>
      <li>Regression: RMSE and MSE</li>
      <li>Regression: MAE</li>
    </ul>
  </li>
</ul>

<h2 id="review-classification-terms">Review Classification Terms</h2>
<ul>
  <li>Based on: <a href="https://www.geeksforgeeks.org/confusion-matrix-machine-learning/">https://www.geeksforgeeks.org/confusion-matrix-machine-learning/</a></li>
  <li>Positive (P) : Observation is positive (for example: is an apple).</li>
  <li>Negative (N) : Observation is not positive (for example: is not an apple).</li>
  <li>True Positive (TP) : Observation is positive, and is predicted to be positive.</li>
  <li>False Negative (FN) : Observation is positive, but is predicted negative.</li>
  <li>True Negative (TN) : Observation is negative, and is predicted to be negative.</li>
  <li>False Positive (FP) : Observation is negative, but is predicted positive.</li>
</ul>

<h2 id="metrics-for-evaluation-roc-curve">Metrics for Evaluation: ROC Curve</h2>
<ul>
  <li>Classification Receiver Operating Characteristic (ROC) curve</li>
  <li>For <font color="red">binary</font> classification (True or False)
    <ul>
      <li>True Positive Rate (TPR) (Sensitivity): True Positive Rate corresponds to the proportion of <font color="red">positive data</font> points that are correctly considered as positive, with respect to all positive data points.</li>
    </ul>
  </li>
</ul>

\[True PositiveRate=\frac{TruePositive}{FalseNegative+TruePositive}\]

<p>False Positive Rate (FPR) (Specificity): False Positive Rate corresponds to the proportion of <font color="red">negative data</font> points that are mistakenly considered as positive, with respect to all negative data points.</p>

\[FalsePositiveRate=\frac{FalsePositive}{FalsePositive+TrueNegative}\]

<p>Based on: <a href="https://towardsdatascience.com/metrics-to-evaluate-your-machine-learning-algorithm-f10ba6e38234">https://towardsdatascience.com/metrics-to-evaluate-your-machine-learning-algorithm-f10ba6e38234</a></p>

<ul>
  <li>FPR and TPR both have values in the range [0, 1]. FPR and TPR bot hare computed at threshold values such as (0.00, 0.02, 0.04, â€¦., 1.00) and a graph is drawn. Area Under Curve (AUC) is the area under the curve of plot FPR vs TPR at different points in [0, 1].
<img src="ROC.png" alt="" /></li>
</ul>

<p>Figure Credit: <a href="https://towardsdatascience.com/metrics-to-evaluate-your-machine-learning-algorithm-f10ba6e38234">https://towardsdatascience.com/metrics-to-evaluate-your-machine-learning-algorithm-f10ba6e38234</a></p>

<ul>
  <li>The greater the value of AUC, the better is the performance of our model.
Each curve is for one class. You may have multiple curves.</li>
</ul>

<h4 id="roc-example">ROC Example</h4>

<h4 id="micro-average-vs-macro-average-in-a-multiclass-classification-problem">Micro Average vs Macro average in a multiclass classification problem:</h4>
<ul>
  <li>A macro-average will compute the metric independently for each class and then take the average (hence treating all classes equally)</li>
  <li>A micro-average will aggregate the contributions of all classes to compute the average metric.</li>
  <li>In a multi-class classification setup, micro-average is preferable if you suspect there might be class imbalance (i.e you may have many more examples of one class than of other classes).</li>
</ul>

<table>
  <tbody>
    <tr>
      <td>Â </td>
      <td>Â </td>
      <td><a href="../../">Index</a></td>
      <td>Â </td>
      <td>Â </td>
      <td><a href="../../">Prev</a></td>
      <td>Â </td>
      <td>Â </td>
      <td><a href="lin-reg2/">Next</a></td>
      <td>Â </td>
      <td>Â </td>
    </tr>
  </tbody>
</table>
:ET
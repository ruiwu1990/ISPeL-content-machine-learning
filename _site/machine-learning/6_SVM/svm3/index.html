<!DOCTYPE html>
<html>
  
  <head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <title>SVM</title>
  <meta name="description" content="ISPeL is an interactive system for personalization of learning. It uses topic-based authoring.">

  <!---   <link rel="stylesheet" href="/ISPeL-content-machine-learning/css/tufte.css" --->
  <link rel="stylesheet" href="../../../css/tufte.css">

  <!-- Google Fonts loaded here depending on setting in _data/options.yml true loads font, blank does not--><link href='//fonts.googleapis.com/css?family=Lato:400,400italic' rel='stylesheet' type='text/css'><!-- Load up MathJax script if needed ... specify in /_data/options.yml file--><script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-AMS_CHTML"></script><script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-75587219-1', 'auto');
  ga('send', 'pageview');
  </script>

  <link rel="canonical" href="http://localhost:7000/ISPeL-content-machine-learning/machine-learning/6_SVM/svm3/">
  <link rel="alternate" type="application/rss+xml" title="ISPeL-content-machine-learning" href="http://localhost:7000/ISPeL-content-machine-learning/feed.xml" />
</head>

  <body>
    <!--- Header and nav template site-wide -->


<header>
    <nav class="group">
    <a href="../../../">Machine Learning</a>
    <a href="http://ispel.cs.ecu.edu/" _target="blank">ISPeL</a>
    <a href="http://seng5005.cs.ecu.edu/" _target="blank">Fall 2020</a>
    <a href="https://github.com/vngudivada/ISPeL-content-machine-learning.git" _target="blank">GitHub</a>
    </nav>
</header>

    <article class="group">
      

<h1>Svm</h1>
<p class="subtitle"></p>


<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    TeX: {
      Macros: {
        e: "\\epsilon",
        xti: "x^{(i)}",
        yti: "y^{(i)}",
        bfy: "{\\bf y}",
        bfx: "{\\bf x}",
        bfg: "{\\bf g}",
        bfbeta: "{\\bf \\beta}",
        tp: "\\tilde p",
        pt: "p_\\theta",
        E: "{\\mathbb{E}}",
        Ind: "{\\mathbb{I}}",
        KL: "{\\mathbb{KL}}",
        Re: "{\\mathbb{R}}",
        Dc: "{\\mathcal{D}}",
        Tc: "{\\mathcal{T}}",
        Xc: "{\\mathcal{X}}",
        note: ["\\textcolor{blue}{[NOTE: #1]}",1]
      }
    }
  });
</script>


<h2 id="support-vector-classifiers">Support Vector Classifiers</h2>
<ul>
  <li>An interesting fact: it turns out that only observations that <font color="red">either lie on the margin or that violate the margin</font> will affect the hyperplane, and hence the classifier obtained. In other words, an observation that lies strictly on the correct side of the margin does not affect the support vector classifier!</li>
  <li>1, 2, 7, 8, 9, 11, 12 affect; 3, 4, 5, 6, 10 does not affect a hyperplane position</li>
  <li>Decision rule is based only on a potentially <font color="red">small subset</font> of the training observations (the support vectors) means that it is quite robust to the behavior of observations that are far away from the hyperplane. Figure 1:</li>
</ul>

<p><label for="SVM" class="margin-toggle">⊕</label>
<input type="checkbox" id="SVM" class="margin-toggle" />
<span class="marginnote"><img class="fullwidth" src="/ISPeL-content-machine-learning/machine-learning/6_SVM/svm3/svc.png" alt="Figure 1" /><br />
    Figure 1
</span></p>

<p><label for="SVM" class="margin-toggle">⊕</label>
<input type="checkbox" id="SVM" class="margin-toggle" />
<span class="marginnote"><img class="fullwidth" src="/ISPeL-content-machine-learning/machine-learning/6_SVM/svm3/svc1.png" alt="What if the classifier is not linear?" /><br />
    What if the classifier is not linear?
</span></p>

<p><label for="SVM" class="margin-toggle">⊕</label>
<input type="checkbox" id="SVM" class="margin-toggle" />
<span class="marginnote"><img class="fullwidth" src="/ISPeL-content-machine-learning/machine-learning/6_SVM/svm3/svm.png" alt="Support vectors are the data points that lie closest to the decision surface. It sounds scary but αi is not 0 only for the &lt;font color=red&gt;support vectors (points on the margin)&lt;/font&gt; Figure credit: https://towardsdatascience.com/support-vector-machine-introduction-to-machine-learning-algorithms-934a444fca47 " /><br />
    Support vectors are the data points that lie closest to the decision surface. It sounds scary but αi is not 0 only for the <font color="red">support vectors (points on the margin)</font> Figure credit: https://towardsdatascience.com/support-vector-machine-introduction-to-machine-learning-algorithms-934a444fca47 
</span></p>

<p><label for="SVM" class="margin-toggle">⊕</label>
<input type="checkbox" id="SVM" class="margin-toggle" />
<span class="marginnote"><img class="fullwidth" src="/ISPeL-content-machine-learning/machine-learning/6_SVM/svm3/kernel.png" alt="Figure 2: Degree of 3, i.e. d= 3" /><br />
    Figure 2: Degree of 3, i.e. d= 3
</span></p>

<p><label for="SVM" class="margin-toggle">⊕</label>
<input type="checkbox" id="SVM" class="margin-toggle" />
<span class="marginnote"><img class="fullwidth" src="/ISPeL-content-machine-learning/machine-learning/6_SVM/svm3/kernel2.png" alt="Figure 3:From left to right: d = 1; d = 3; d = 5. The higher d means higher dimensional hyperplane (non-linear), a more flexible (curvy) decision boundary. Figure credit: https://yunhaocsblog.wordpress.com/2014/07/27/the-effects-of-hyperparameters-in-svm/" /><br />
    Figure 3:From left to right: d = 1; d = 3; d = 5. The higher d means higher dimensional hyperplane (non-linear), a more flexible (curvy) decision boundary. Figure credit: https://yunhaocsblog.wordpress.com/2014/07/27/the-effects-of-hyperparameters-in-svm/
</span></p>

<ul>
  <li>this is an optimization problem for quadratic relation:</li>
</ul>

<p>maximize M</p>

\[\beta_0,\beta_{11},\beta_{12}....,\beta_{p1},\beta_{p2}, \epsilon _1,..., \epsilon_n,M\]

<p>subject to</p>

\[y_i(\beta_0+ \Sigma_{j=1}^p\beta_{j1}x_{ij}+\Sigma_{j=1}^p\beta_{j2}x_{ij}^2) \geq M(1- \epsilon _i),  \Sigma _{i=1}^n \epsilon _i \leq C, \epsilon _i \geq 0, \Sigma _{j=1}^p \Sigma _{k=1}^2\beta_{jk}^2=1.\]

<ul>
  <li>If you want to know why and how to turn the problem into math above: <a href="https://www.youtube.com/watch?v=_PwhiWxHK8o">https://www.youtube.com/watch?v=_PwhiWxHK8o</a></li>
</ul>

<h2 id="support-vector-machine-generalize-idea">Support Vector Machine: Generalize Idea</h2>
<ul>
  <li>SVM is the general answer for <font color="red">non-linear classifier</font>.</li>
  <li>SVM an extension of the support vector classifier that results from enlarging the feature space in a specific way, using <font color="red">kernels</font>.</li>
  <li>The details on explanations are very complex and beyond the scope of this class, but the key idea is introduced in the previous slides: only involves inner product of vectors (xi is training data, x is data point we want to predict):</li>
</ul>

\[f(x) = \beta _0+ \Sigma _{i=1}^n \alpha _i \big\{x,x_i\big\}\]

\[\big\{x,x_{i\prime} \big\}= \Sigma_{j=1}^px_{ij}x_{i\prime}j\]

<ul>
  <li>where there are n parameters αi, i = 1, . . . , n, one per training observation.</li>
  <li>This means if you have 10,000 training data records, you will have 10,000 α. In fact, most α will be 0.</li>
  <li>To estimate the parameters α1, . . . , αn and β0, all we need are the <font color="red">n(n − 1)/2</font> inner products &lt;xi, xi’&gt;  between all pairs of training observations.</li>
  <li>Support vectors are the data points that lie closest to the decision surface.</li>
  <li>
    <p>It sounds scary but αi is not 0 only for the <font color="red">support vectors (points on the margin)</font></p>
  </li>
  <li>αi is not 0 only for the <font color="red">support vectors</font></li>
  <li>Because of this,</li>
</ul>

\[f(x) = \beta _0+ \Sigma _{i=1}^n \alpha _i \big\{x,x_i\big\}\]

\[TO\]

\[f(x) = \beta _0+ \Sigma _{i\epsilon S} \alpha _i K(x,x_i)\]

<ul>
  <li>S is the support vector collection. The xi, xi’ is called <font color="red">kernel</font> and can be written as K(xi, xi’).</li>
  <li>Inner Kernel:</li>
</ul>

\[K(x_i,x_{i\prime})\Sigma _{j=1}^p x_{ij}x_{i\prime}j\]

<ul>
  <li>The <font color="red">linear kernel</font> essentially quantifies the similarity of a pair of observations using Pearson (standard) correlation.</li>
  <li>
    <p>Question: What is Pearson correlation?</p>
  </li>
  <li>Besides linear kernel, there are some other popular kernels, e.g. <font color="red">polynomial</font> kernel and <font color="red">radial</font> kernel.</li>
  <li>Let’s have a closer look at polynomial kernel first:</li>
</ul>

\[K(x_i,x_{i\prime})\Sigma _{j=1}^p x_{ij}x_{i\prime}j\]

<ul>
  <li>d is a positive integer. It essentially amounts to fitting a support vector classifier in a <font color="red">higher-dimensional</font> space involving polynomials of degree <font color="red">d</font>, rather than in the original feature space. Example Figure 2 and 3.</li>
</ul>

<table>
  <tbody>
    <tr>
      <td> </td>
      <td><a href="../../../">Index</a></td>
      <td> </td>
      <td> </td>
      <td><a href="../svm2">Prev</a></td>
      <td> </td>
      <td> </td>
      <td><a href="../svm4">Next</a></td>
      <td> </td>
      <td> </td>
    </tr>
  </tbody>
</table>



    </article>
    <footer>
<hr class="slender">
<div class="credits">
This work is supported by the <a href="https://www.nsf.gov/" target="_blank">NSF</a> IUSE/PFE:RED award No. <a href="https://www.nsf.gov/awardsearch/showAward?AWD_ID=1730568" target="_blank">1730568</a>. Site created with <a href="//jekyllrb.com" target="_blank">Jekyll</a> using the <a href="//github.com/clayh53/tufte-jekyll" target="_blank">Tufte theme</a>. &copy; 2020
</div>
</footer>
  </body>
</html>

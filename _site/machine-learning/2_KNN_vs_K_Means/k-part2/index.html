<!DOCTYPE html>
<html>
  
  <head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <title>KNN</title>
  <meta name="description" content="ISPeL is an interactive system for personalization of learning. It uses topic-based authoring.">

  <!---   <link rel="stylesheet" href="/ISPeL-content-machine-learning/css/tufte.css" --->
  <link rel="stylesheet" href="../../../css/tufte.css">

  <!-- Google Fonts loaded here depending on setting in _data/options.yml true loads font, blank does not--><link href='//fonts.googleapis.com/css?family=Lato:400,400italic' rel='stylesheet' type='text/css'><!-- Load up MathJax script if needed ... specify in /_data/options.yml file--><script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-AMS_CHTML"></script><script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-75587219-1', 'auto');
  ga('send', 'pageview');
  </script>

  <link rel="canonical" href="http://localhost:7000/ISPeL-content-machine-learning/machine-learning/2_KNN_vs_K_Means/k-part2/">
  <link rel="alternate" type="application/rss+xml" title="ISPeL-content-machine-learning" href="http://localhost:7000/ISPeL-content-machine-learning/feed.xml" />
</head>

  <body>
    <!--- Header and nav template site-wide -->


<header>
    <nav class="group">
    <a href="../../../">Machine Learning</a>
    <a href="http://ispel.cs.ecu.edu/" _target="blank">ISPeL</a>
    <a href="http://seng5005.cs.ecu.edu/" _target="blank">Fall 2020</a>
    <a href="https://github.com/vngudivada/ISPeL-content-machine-learning.git" _target="blank">GitHub</a>
    </nav>
</header>

    <article class="group">
      

<h1>Knn</h1>
<p class="subtitle"></p>


<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    TeX: {
      Macros: {
        e: "\\epsilon",
        xti: "x^{(i)}",
        yti: "y^{(i)}",
        bfy: "{\\bf y}",
        bfx: "{\\bf x}",
        bfg: "{\\bf g}",
        bfbeta: "{\\bf \\beta}",
        tp: "\\tilde p",
        pt: "p_\\theta",
        E: "{\\mathbb{E}}",
        Ind: "{\\mathbb{I}}",
        KL: "{\\mathbb{KL}}",
        Re: "{\\mathbb{R}}",
        Dc: "{\\mathcal{D}}",
        Tc: "{\\mathcal{T}}",
        Xc: "{\\mathcal{X}}",
        note: ["\\textcolor{blue}{[NOTE: #1]}",1]
      }
    }
  });
</script>


<ul>
  <li>KNN is supervised learning</li>
  <li>KNN can be a classification technique and also used for regression.</li>
  <li>Sklearn:
    <ul>
      <li>KNeighborsClassifier: classification</li>
      <li>KNeighborsRegressor: regression</li>
    </ul>
  </li>
</ul>

<p><img src="knn2.png" alt="" /></p>

<p>Figure Credit: <a href="https://www.mathworks.com/matlabcentral/fileexchange/63621-knn-classifier">https://www.mathworks.com/matlabcentral/fileexchange/63621-knn-classifier</a></p>

<h3 id="here-are-some-challenges">Here are some challenges:</h3>
<ul>
  <li>How to calculate the similarity (e.g., friends): distances between each data point, Euclidean distance.</li>
  <li>How to choose K: if we need to classify students into groups, then how to decide the number of K. ATTENSION: K represents how many neighbors you want to consider for the response; K does not mean number of groups.</li>
</ul>

<p><img src="knn3.png" alt="" /></p>

<p>Figure Credit: <a href="https://aws.amazon.com/blogs/machine-learning/amazon-sagemaker-supports-knn-classification-and-regression/">https://aws.amazon.com/blogs/machine-learning/amazon-sagemaker-supports-knn-classification-and-regression/</a></p>

<h3 id="knn-how">KNN: How</h3>
<ul>
  <li>The KNN algorithm is belongs to the family of instance-based, competitive learning and lazy learning algorithms.</li>
  <li>Instance-based algorithms are those algorithms that model the problem using data instances (or rows) in order to make predictive decisions. The KNN algorithm is an extreme form of instance-based methods because all training observations are retained as part of the model, i.e. without data, your KNN model does not exist.</li>
  <li>It is a competitive learning algorithm, because it internally uses competition between model elements (data instances) in order to make a predictive decision. The objective similarity measure between data instances causes each data instance to compete to “win” or be most similar to a given unseen data instance and contribute to a prediction.</li>
  <li>Lazy learning refers to the fact that the algorithm does not build a model until the time that a prediction is required. It is lazy because it only does work at the last second. This has the benefit of only including data relevant to the unseen data, called a localized model. A disadvantage is that it can be computationally expensive to repeat the same or similar searches over larger training datasets.</li>
  <li>Finally, KNN is powerful because it does not assume anything about the data, other than a distance measure can be calculated consistently between any two instances. As such, it is called non-parametric or non-linear as it does not assume a functional form.</li>
</ul>

<h1 id="knn-advantages-vs-disadvantages">KNN Advantages vs Disadvantages</h1>

<h4 id="advantages">Advantages:</h4>
<ul>
  <li>You do not need to make assumptions about data (i.e., linear vs nonlinear, normal distribution…)</li>
  <li>KNN is simple compared with other algorithm.</li>
  <li>Usually, KNN performs not bad</li>
  <li>KNN is robust to noisy training data, a few noise data points will not impact results a lot.
…</li>
</ul>

<h4 id="disadvantages">Disadvantages:</h4>
<ul>
  <li>You need to determine K.</li>
  <li>KNN is computationally expensive. It requires many calculations—distances.</li>
  <li>KNN requires a lot of memory.</li>
  <li>KNN needs to use all training data.
…</li>
</ul>

<p><img src="knn2.png" alt="" />
Figure Credit: <a href="https://www.mathworks.com/matlabcentral/fileexchange/63621-knn-classifier">https://www.mathworks.com/matlabcentral/fileexchange/63621-knn-classifier</a></p>

<h3 id="knn-k-changes">KNN: K changes</h3>

<p><img src="knn4.png" alt="" /></p>
<ul>
  <li>K-means clustering aims to partition n observations into k clusters in which each observation belongs to the cluster with the nearest mean.</li>
  <li>If you want to separate different car models into 4 categories based on horsepower, engine displacement, and MPG, you can use K-means.</li>
  <li>KNN is unsupervised learning and a clustering algorithm (involves the grouping of data points).</li>
  <li>K represents # of centroids.
<img src="k-means.png" alt="" /></li>
</ul>

<p>Figure Credit: https://blogs.oracle.com/bigdata/k-means-clustering-machine-learning</p>

<h4 id="an-example">An Example</h4>
<p><img src="k-means2.png" alt="" /></p>

<p>Example is based on: https://www.youtube.com/watch?v=4b5d3muPQmA</p>

<h3 id="k-means-advantages-vs-disadvantages">K-Means: Advantages vs Disadvantages</h3>
<h4 id="advantages-1">Advantages:</h4>
<ul>
  <li>Easy to implement.</li>
  <li>K-means model dynamically updated: centroid can be updated if new dataset is added.</li>
</ul>

<h4 id="disadvantages-1">Disadvantages:</h4>
<ul>
  <li>Hard to guess K</li>
  <li>Initial centroid can impact results</li>
  <li>K-means is time consuming. Need to calculate distances between new centroid in every loop.</li>
  <li>K-means method may not find out the global best solution. It sometimes returns local optimum.</li>
  <li>K-means is limited to linear cluster boundaries.</li>
</ul>

<table>
  <tbody>
    <tr>
      <td> </td>
      <td><a href="../../">Index</a></td>
      <td> </td>
      <td> </td>
      <td><a href="../">Prev</a></td>
      <td> </td>
      <td> </td>
      <td><a href="../k-part3/">Next</a></td>
      <td> </td>
      <td> </td>
    </tr>
  </tbody>
</table>


    </article>
    <footer>
<hr class="slender">
<div class="credits">
This work is supported by the <a href="https://www.nsf.gov/" target="_blank">NSF</a> IUSE/PFE:RED award No. <a href="https://www.nsf.gov/awardsearch/showAward?AWD_ID=1730568" target="_blank">1730568</a>. Site created with <a href="//jekyllrb.com" target="_blank">Jekyll</a> using the <a href="//github.com/clayh53/tufte-jekyll" target="_blank">Tufte theme</a>. &copy; 2020
</div>
</footer>
  </body>
</html>

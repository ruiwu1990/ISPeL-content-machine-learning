<!DOCTYPE html>
<html>
  
  <head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <title>K-Means</title>
  <meta name="description" content="ISPeL is an interactive system for personalization of learning. It uses topic-based authoring.">

  <!---   <link rel="stylesheet" href="/ISPeL-content-machine-learning/css/tufte.css" --->
  <link rel="stylesheet" href="../../../css/tufte.css">

  <!-- Google Fonts loaded here depending on setting in _data/options.yml true loads font, blank does not--><link href='//fonts.googleapis.com/css?family=Lato:400,400italic' rel='stylesheet' type='text/css'><!-- Load up MathJax script if needed ... specify in /_data/options.yml file--><script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-AMS_CHTML"></script><script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-75587219-1', 'auto');
  ga('send', 'pageview');
  </script>

  <link rel="canonical" href="http://localhost:7000/ISPeL-content-machine-learning/machine-learning/2_KNN_vs_K_Means/k-part4/">
  <link rel="alternate" type="application/rss+xml" title="ISPeL-content-machine-learning" href="http://localhost:7000/ISPeL-content-machine-learning/feed.xml" />
</head>

  <body>
    <!--- Header and nav template site-wide -->


<header>
    <nav class="group">
    <a href="../../../">Machine Learning</a>
    <a href="http://ispel.cs.ecu.edu/" _target="blank">ISPeL</a>
    <a href="http://seng5005.cs.ecu.edu/" _target="blank">Fall 2020</a>
    <a href="https://github.com/vngudivada/ISPeL-content-machine-learning.git" _target="blank">GitHub</a>
    </nav>
</header>

    <article class="group">
      

<h1>K-means</h1>
<p class="subtitle"></p>


<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    TeX: {
      Macros: {
        e: "\\epsilon",
        xti: "x^{(i)}",
        yti: "y^{(i)}",
        bfy: "{\\bf y}",
        bfx: "{\\bf x}",
        bfg: "{\\bf g}",
        bfbeta: "{\\bf \\beta}",
        tp: "\\tilde p",
        pt: "p_\\theta",
        E: "{\\mathbb{E}}",
        Ind: "{\\mathbb{I}}",
        KL: "{\\mathbb{KL}}",
        Re: "{\\mathbb{R}}",
        Dc: "{\\mathcal{D}}",
        Tc: "{\\mathcal{T}}",
        Xc: "{\\mathcal{X}}",
        note: ["\\textcolor{blue}{[NOTE: #1]}",1]
      }
    }
  });
</script>


<ul>
  <li>Kmeans clustering aims to partition n observations into k clusters in which each observation belongs to the cluster with the <mark>nearest mean</mark>.</li>
  <li>If you want to separate different car models into 4 categories based on horsepower, engine displacement, and MPG, you can use K-means.</li>
  <li><mark>Question: is Kmeans supervised or unsupervised learning?</mark></li>
  <li>K represents # of centroids.
<img src="k-means.png" alt="" /></li>
</ul>

<p>Figure Credit: <a href="https://blogs.oracle.com/bigdata/k-means-clustering-machine-learning">https://blogs.oracle.com/bigdata/k-means-clustering-machine-learning</a></p>

<h3 id="k-means-how-does-it-work">K-Means: How does it work?</h3>
<ul>
  <li>Here are K-means algorithm steps:
    <ul>
      <li>Step 1: Choose a K: it represents the number of centroids. Hard to decide… especially for high dimensional data. You will learn how to do this in your homework 2.</li>
      <li>Step 2: Randomly selected centroids positions</li>
      <li>Step 3: Calculate distances between each data point with K centroids and decide cluster response (closer)</li>
      <li>Step 4: Calculate the mean value of each cluster, use the mean value as the new centroid</li>
      <li>Step 5: repeat step 3 and 4 until you have repeated N (maybe 100) times or centroids do not change anymore.</li>
    </ul>
  </li>
  <li>
    <p>Kmeans is unsupervised learning and a clustering algorithm (involves the grouping of data points).</p>
  </li>
  <li>The following figure shows how we decide which group a data point belongs to.
<img src="km3.png" alt="" /></li>
  <li>The image below is a good cluster result using Kmeans.
<img src="km4.png" alt="" />
Figure Credit: <a href="https://blogs.oracle.com/bigdata/k-means-clustering-machine-learning">https://blogs.oracle.com/bigdata/k-means-clustering-machine-learning</a></li>
</ul>

<h3 id="an-example">An Example</h3>
<ul>
  <li>Let’s tale a closer look at the following 2D example and review the steps.
<img src="k-means2.png" alt="" /></li>
  <li>Things can go wrong like the image below:
<img src="k-means3.png" alt="" /></li>
  <li><mark>Question: how to avoid this?</mark></li>
  <li>Same steps for higher dimensional data.</li>
</ul>

<p>Example is based on: https://www.youtube.com/watch?v=4b5d3muPQmA</p>

<h3 id="group-activity">Group Activity</h3>
<ul>
  <li>Group Activity 3: <a href="https://github.com/ruiwu1990/CSCI_4120/blob/master/K-Means/Group%20Activity%203.ipynb">https://github.com/ruiwu1990/CSCI_4120/blob/master/K-Means/Group%20Activity%203.ipynb</a></li>
  <li>Finish to do section, you will have the K-means implementation from scratch.</li>
  <li>Seaborn is a Python data visualization library based on matplotlib. It provides a high-level interface for drawing attractive and informative statistical graphics.</li>
  <li>pairwise_distances_argmin: Compute minimum distances between one point and a set of points, similar to finding neighbors function from KNN</li>
  <li>Kmeans(k, random_state): k how many neighbours, random_state determines random number generatation for centroid initialization</li>
  <li>How to cluster non-linear data? Map your data into a higher dimension and apply k-means. In sklearn: SpectralClustering.</li>
</ul>

<h3 id="k-means-advantages-vs-disadvantages">K-Means: Advantages vs Disadvantages</h3>
<h4 id="advantages">Advantages:</h4>
<ul>
  <li>Easy to implement.</li>
  <li>K-means model dynamically updated: centroid can be updated if new dataset is added.</li>
</ul>

<h4 id="disadvantages">Disadvantages:</h4>
<ul>
  <li>Hard to guess K</li>
  <li>Initial centroid can impact results</li>
  <li>K-means is time consuming. Need to calculate distances between new centroid in every loop.</li>
  <li>K-means method may not find out the global best solution. It sometimes returns local optimum.</li>
  <li>K-means is limited to linear cluster boundaries.</li>
</ul>

<h3 id="classification-definition-of-terms">Classification Definition of Terms</h3>
<ul>
  <li>Based on: <a href="https://www.geeksforgeeks.org/confusion-matrix-machine-learning/">https://www.geeksforgeeks.org/confusion-matrix-machine-learning/</a></li>
  <li><mark>Question: how to judge if a classification results are good or not.</mark></li>
  <li>Need to learn the following concepts and then we can answer the question:
    <ul>
      <li>Positive (P) : Observation is positive (for example: is an apple).</li>
      <li>Negative (N) : Observation is not positive (for example: is not an apple).</li>
      <li>True Positive (TP) : Observation is positive, and is predicted to be positive.</li>
      <li>False Negative (FN) : Observation is positive, but is predicted negative.</li>
      <li>True Negative (TN) : Observation is negative, and is predicted to be negative.</li>
      <li>False Positive (FP) : Observation is negative, but is predicted positive.</li>
    </ul>
  </li>
</ul>

<h3 id="confusion-matrix">Confusion Matrix</h3>
<ul>
  <li>Used for classification results.</li>
  <li>A confusion matrix is a summary of prediction results on a classification problem.</li>
  <li>The number of correct and incorrect predictions are summarized with count values and broken down by each class. This is the key to the confusion matrix.</li>
  <li>The confusion matrix shows the ways in which your classification model is confused when it makes predictions.</li>
  <li>It gives us insight not only into the errors being made by a classifier but more importantly the types (to be learned) of errors that are being made.</li>
  <li>An example of 2-class classification problem (image below)
    <ul>
      <li>Class 1 : Positive</li>
      <li>Class 2 : Negative</li>
      <li><img src="km5.png" alt="" /></li>
    </ul>
  </li>
  <li>Figure credit: <a href="https://www.geeksforgeeks.org/confusion-matrix-machine-learning">https://www.geeksforgeeks.org/confusion-matrix-machine-learning</a></li>
</ul>

<h3 id="k-means-advanced-examples">K-Means: Advanced Examples</h3>
<ul>
  <li>Sample Code: <a href="https://github.com/ruiwu1990/CSCI_4120/blob/master/K-Means/05.11-K-Means.ipynb">https://github.com/ruiwu1990/CSCI_4120/blob/master/K-Means/05.11-K-Means.ipynb</a></li>
  <li>Example 1 Handwriting digit recognition:
    <ul>
      <li>8*8 data matrix to represent a digit handwriting image</li>
      <li>Each image is a data point and the centroid of each cluster is also an image</li>
      <li>Matplotlib imshow: render a 2D regular image</li>
      <li>np.zero_like(clusters): create a matrix with all zeros, the shape is the same as the “clusters” matrix.</li>
      <li>TSNE: a method to reduce dimension, it models each high-dimensional object by a two- or three-dimensional point in such a way that similar objects are modeled by nearby points and dissimilar objects are modeled by distant points with high probability.</li>
    </ul>
  </li>
  <li>Example 2 Color Compression
    <ul>
      <li>numpy.random.permutation: Randomly permute a sequence, or return a permuted range; same numbers but different orders</li>
      <li>Matrix.T: .T is to transpose a numpy matrix</li>
      <li>MiniBatchKMeans: calculate distances between the centroid and only a random sample of observations as opposed to all observations. Faster but may perform not as good as traditional K-Means.</li>
    </ul>
  </li>
</ul>

<h3 id="k-means-how-to-find-a-good-k">K-Means: how to find a good K</h3>
<ul>
  <li>How to judge if a K-means algorithm is good?
    <ul>
      <li>First you need a measure: one of the most popular one is named “distortion” or “SSE” based on sum of squared errors (SSE), (datapoint - centroid)^2.</li>
      <li>Compare distortion with K. The best K is at the rapid change point, creating elbow shape.</li>
      <li>This method is usually called “elbow” method because the shape is like an elbow.</li>
      <li><mark>Question: why larger K, less distortion?</mark></li>
      <li><img src="km6.png" alt="" /></li>
      <li>Figure credit: <a href="https://bl.ocks.org/rpgove/0060ff3b656618e9136b">https://bl.ocks.org/rpgove/0060ff3b656618e9136b</a></li>
      <li><img src="km7.png" alt="" /></li>
      <li>Figure credit: <a href="https://www.dreamstime.com/elbow-joint-vector-illustrated-diagram-medical-scheme-educational-sports-injury-information-elbow-joint-vector-illustrated-diagram-image108866773">https://www.dreamstime.com/elbow-joint-vector-illustrated-diagram-medical-scheme-educational-sports-injury-information-elbow-joint-vector-illustrated-diagram-image108866773</a></li>
      <li>Group Activity 4: <a href="https://github.com/ruiwu1990/CSCI_4120/blob/master/K-Means/Group%20Activity%204.ipynb">https://github.com/ruiwu1990/CSCI_4120/blob/master/K-Means/Group%20Activity%204.ipynb</a></li>
    </ul>
  </li>
</ul>

<h3 id="group-homework-2-find-an-appropriate-k-for-k-means">Group Homework 2: Find an Appropriate K for K-means</h3>
<ul>
  <li>Goal: learn how to find a good K using Elbow Method library</li>
  <li>You need to use the command “python3 hw2.py” to check the outputs</li>
  <li>Complete todo section in HW2: <a href="https://github.com/ruiwu1990/CSCI_4120/tree/master/HW_elbow_kmeans">https://github.com/ruiwu1990/CSCI_4120/tree/master/HW_elbow_kmeans</a></li>
  <li>Use KElbowVisualizer to decide K for K-means
    <ul>
      <li>Install:</li>
      <li>pip3 install yellowbrick</li>
      <li>Read document about the function: <a href="https://www.scikit-yb.org/en/latest/api/cluster/elbow.html">https://www.scikit-yb.org/en/latest/api/cluster/elbow.html</a></li>
    </ul>
  </li>
  <li>Draw a Confusion Matrix</li>
  <li>README.MD file
    <ul>
      <li>Team member names and email addresses</li>
      <li>Quick Start</li>
      <li>K*K matrix, K is decided in by KElbowVisualizer</li>
      <li>Which K works the best</li>
      <li>The best K accuracy</li>
      <li>Insert a confusion matrix for the best K</li>
    </ul>
  </li>
  <li><mark>Due September 28</mark>.</li>
</ul>

<table>
  <tbody>
    <tr>
      <td> </td>
      <td><a href="../../../">Index</a></td>
      <td> </td>
      <td> </td>
      <td><a href="../k-part3/">Prev</a></td>
      <td> </td>
      <td> </td>
    </tr>
  </tbody>
</table>


    </article>
    <footer>
<hr class="slender">
<div class="credits">
This work is supported by the <a href="https://www.nsf.gov/" target="_blank">NSF</a> IUSE/PFE:RED award No. <a href="https://www.nsf.gov/awardsearch/showAward?AWD_ID=1730568" target="_blank">1730568</a>. Site created with <a href="//jekyllrb.com" target="_blank">Jekyll</a> using the <a href="//github.com/clayh53/tufte-jekyll" target="_blank">Tufte theme</a>. &copy; 2021
</div>
</footer>
  </body>
</html>

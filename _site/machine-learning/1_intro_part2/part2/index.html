<!DOCTYPE html>
<html>
  
  <head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <title>Statistical Learning</title>
  <meta name="description" content="ISPeL is an interactive system for personalization of learning. It uses topic-based authoring.">

  <!---   <link rel="stylesheet" href="/ISPeL-content-machine-learning/css/tufte.css" --->
  <link rel="stylesheet" href="../../../css/tufte.css">

  <!-- Google Fonts loaded here depending on setting in _data/options.yml true loads font, blank does not--><link href='//fonts.googleapis.com/css?family=Lato:400,400italic' rel='stylesheet' type='text/css'><!-- Load up MathJax script if needed ... specify in /_data/options.yml file--><script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-AMS_CHTML"></script><script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-75587219-1', 'auto');
  ga('send', 'pageview');
  </script>

  <link rel="canonical" href="http://localhost:7000/ISPeL-content-machine-learning/machine-learning/1_intro_part2/part2/">
  <link rel="alternate" type="application/rss+xml" title="ISPeL-content-machine-learning" href="http://localhost:7000/ISPeL-content-machine-learning/feed.xml" />
</head>

  <body>
    <!--- Header and nav template site-wide -->


<header>
    <nav class="group">
    <a href="../../../">Machine Learning</a>
    <a href="http://ispel.cs.ecu.edu/" _target="blank">ISPeL</a>
    <a href="http://seng5005.cs.ecu.edu/" _target="blank">Fall 2020</a>
    <a href="https://github.com/vngudivada/ISPeL-content-machine-learning.git" _target="blank">GitHub</a>
    </nav>
</header>

    <article class="group">
      

<h1>Statistical learning</h1>
<p class="subtitle"></p>


<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    TeX: {
      Macros: {
        e: "\\epsilon",
        xti: "x^{(i)}",
        yti: "y^{(i)}",
        bfy: "{\\bf y}",
        bfx: "{\\bf x}",
        bfg: "{\\bf g}",
        bfbeta: "{\\bf \\beta}",
        tp: "\\tilde p",
        pt: "p_\\theta",
        E: "{\\mathbb{E}}",
        Ind: "{\\mathbb{I}}",
        KL: "{\\mathbb{KL}}",
        Re: "{\\mathbb{R}}",
        Dc: "{\\mathcal{D}}",
        Tc: "{\\mathcal{T}}",
        Xc: "{\\mathcal{X}}",
        note: ["\\textcolor{blue}{[NOTE: #1]}",1]
      }
    }
  });
</script>


<h3 id="the-regression-function-fx">The regression function f(x)</h3>

<ul>
  <li>Is also deﬁned for vector \(X\) ; e.g.</li>
</ul>

\[f(x) = f(x_1,x_2,x_3)=E(Y|X_1=x_1, X_2 = x_2, X_3 = x_3)\]

<ul>
  <li>Is the ideal or optimal predictor of \(Y\) with regard to mean-squared prediction error: 
\(f(x) = E(Y|X = x)\) is the function that minimizes \(E\{Y-g(X))^2|X=x\}\) over all functions \(g\) at all points \(X = x\).</li>
  <li>\(\epsilon=Y-f(x)\) is the <span style="background-color: #FFFF00">irreducible error</span> — i.e. even if we knew \(f(x)\), we would still make errors in prediction, since at each \(X = x\) there is typically a distribution of possible \($Y\) values.</li>
  <li>For any estimate \(\hat{f}(x)\) of \(f(x)\), we have our errors:
 \(E[(Y-\hat{f}(X))^2|X=x]=[f(x)-\hat{f}(x)]^2+Var(\epsilon)\)</li>
</ul>

<h3 id="how-to-estimate-f">How to estimate f</h3>

<p>Typically we have few if any data points with \(X = 4\) exactly.
So we cannot compute \(E(Y|X = x)!\) 
Relax the deﬁnition and let \(\hat{f}(x)=Ave(Y|X\epsilon N(x))\) where \(N(x)\) is some neighborhood of \(x\).</p>

<p><img src="howfx.png" alt="" /></p>

<ul>
  <li>Nearest neighbor averaging can be pretty good for small \(p\) (number of features X) — i.e. \(p ≤ 4\).</li>
  <li>We will discuss some techniques, such as kernel and spline smoothing later in the course.</li>
  <li>Nearest neighbor methods can be not effective when p is large. Reason: the curse of dimensionality. Nearest neighbors tend to be far away in high dimensions.
– We need to get a reasonable fraction of the \(N\) values of \(y_i\) to average to bring the variance down—e.g. 10%.
– A 10% neighborhood in high dimensions need no longer be local, so we lose the spirit of estimating \(E(Y|X = x)\) by local averaging.</li>
</ul>

<h3 id="curse-of-dimensionality">Curse of Dimensionality</h3>

<p><img src="curse.png" alt="" /></p>

<h3 id="sample-code-curse-of-dimension">Sample Code: curse of dimension</h3>
<ul>
  <li>Higher dimensions -&gt; more troubles</li>
  <li><a href="https://github.com/ruiwu1990/CSCI_4120/tree/master/curse_of_demensionality">https://github.com/ruiwu1990/CSCI_4120/tree/master/curse_of_demensionality</a></li>
</ul>

<table>
  <tbody>
    <tr>
      <td> </td>
      <td> </td>
      <td><a href="../../">Index</a></td>
      <td> </td>
      <td> </td>
      <td><a href="../">Prev</a></td>
      <td> </td>
      <td> </td>
    </tr>
  </tbody>
</table>


    </article>
    <footer>
<hr class="slender">
<div class="credits">
This work is supported by the <a href="https://www.nsf.gov/" target="_blank">NSF</a> IUSE/PFE:RED award No. <a href="https://www.nsf.gov/awardsearch/showAward?AWD_ID=1730568" target="_blank">1730568</a>. Site created with <a href="//jekyllrb.com" target="_blank">Jekyll</a> using the <a href="//github.com/clayh53/tufte-jekyll" target="_blank">Tufte theme</a>. &copy; 2020
</div>
</footer>
  </body>
</html>

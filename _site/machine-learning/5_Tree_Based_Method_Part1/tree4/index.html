<!DOCTYPE html>
<html>
  
  <head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <title>Tree Based Method</title>
  <meta name="description" content="ISPeL is an interactive system for personalization of learning. It uses topic-based authoring.">

  <!---   <link rel="stylesheet" href="/ISPeL-content-machine-learning/css/tufte.css" --->
  <link rel="stylesheet" href="../../../css/tufte.css">

  <!-- Google Fonts loaded here depending on setting in _data/options.yml true loads font, blank does not--><link href='//fonts.googleapis.com/css?family=Lato:400,400italic' rel='stylesheet' type='text/css'><!-- Load up MathJax script if needed ... specify in /_data/options.yml file--><script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-AMS_CHTML"></script><script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-75587219-1', 'auto');
  ga('send', 'pageview');
  </script>

  <link rel="canonical" href="http://localhost:7000/ISPeL-content-machine-learning/machine-learning/5_Tree_Based_Method_Part1/tree4/">
  <link rel="alternate" type="application/rss+xml" title="ISPeL-content-machine-learning" href="http://localhost:7000/ISPeL-content-machine-learning/feed.xml" />
</head>

  <body>
    <!--- Header and nav template site-wide -->


<header>
    <nav class="group">
    <a href="../../../">Machine Learning</a>
    <a href="http://ispel.cs.ecu.edu/" _target="blank">ISPeL</a>
    <a href="http://seng5005.cs.ecu.edu/" _target="blank">Fall 2020</a>
    <a href="https://github.com/vngudivada/ISPeL-content-machine-learning.git" _target="blank">GitHub</a>
    </nav>
</header>

    <article class="group">
      

<h1>Tree based method</h1>
<p class="subtitle"></p>


<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    TeX: {
      Macros: {
        e: "\\epsilon",
        xti: "x^{(i)}",
        yti: "y^{(i)}",
        bfy: "{\\bf y}",
        bfx: "{\\bf x}",
        bfg: "{\\bf g}",
        bfbeta: "{\\bf \\beta}",
        tp: "\\tilde p",
        pt: "p_\\theta",
        E: "{\\mathbb{E}}",
        Ind: "{\\mathbb{I}}",
        KL: "{\\mathbb{KL}}",
        Re: "{\\mathbb{R}}",
        Dc: "{\\mathcal{D}}",
        Tc: "{\\mathcal{T}}",
        Xc: "{\\mathcal{X}}",
        note: ["\\textcolor{blue}{[NOTE: #1]}",1]
      }
    }
  });
</script>


<h2 id="decision-treean-example">Decision Tree—An Example</h2>

<p><img src="dtree.png" alt="" /></p>
<ul>
  <li>
    <p>Example is from: <a href="https://www.xoriant.com/blog/product-engineering/decision-trees-machine-learning-algorithm.html">https://www.xoriant.com/blog/product-engineering/decision-trees-machine-learning-algorithm.html</a></p>
  </li>
  <li>Compare performance of different columns</li>
  <li>Performance is calculated using Entropy (purity) and Information Gain (IG) (i.e. parents - children)</li>
  <li>Higher IG means better</li>
  <li>P(x) is the probability (proportion) of event x</li>
  <li>H(S) entropy for what we want to classify (i.e. parent)</li>
</ul>

\[H(S) =  \Sigma_{x \epsilon X}p(x)log_2 \frac{1}{p(x)}\]

\[IG(S,A)=H(S) -  \Sigma _{i=0}^np(x)*H(x)\]

\[Entropy(S) =  \Sigma_{x \epsilon X}p(x)log_2 \frac{1}{p(x)}\]

<p>\(Entropy(S) = - ( \frac{9}{14})log_2( \frac{9}{14}-( \frac{5}{14})log_2(\frac{5}{14})=0.940\)</p>
<ul>
  <li>Play Golf: 5 “no”, 9 “yes”</li>
</ul>

<h3 id="decision-tree-wind">Decision Tree: “Wind”</h3>
<ul>
  <li>H(S) = 0.94</li>
</ul>

\[IG(S,Wind)=H(S) -  \Sigma _{i=0}^np(x)*H(x)\]

\[P(S_{weak})=\frac{Number of WeaK}{Total}\]

\[P(S_{strong})=\frac{Number of Strong}{Total}\]

\[Entropy(S_{weak}) = - ( \frac{6}{8})log_2( \frac{6}{8}-( \frac{2}{8})log_2(\frac{2}{8})= 0.811\]

\[Entropy(S_{strong}) = - ( \frac{3}{6})log_2( \frac{3}{6}-( \frac{3}{6})log_2(\frac{3}{6})=1.000\]

\[IG(S,Wind)=H(S) -  \Sigma _{i=0}^np(x)*H(x)\]

\[IG(S,Wind)=H(S)-P(S_{weak})*H(S_{weak})-P(S_{strong})*H(S_{strong})\]

\[= 0.940-(\frac{8}{14})(0.811)-(\frac{6}{14})(1.00) = 0.048\]

<h2 id="decision-tree">Decision Tree</h2>
<ul>
  <li>Choose column “Outlook”</li>
  <li>“Sunny” branch</li>
  <li>Keep divide data for each branch</li>
</ul>

<p><img src="dtree1.png" alt="" /></p>

<p>\(IG(S,Outlook) = 0.246\)
\(IG(S,Temperature) = 0.029\)
\(IG(S,Humidity) = 0.151\)
\(IG(S,Wind) = 0.048 (Previous example)\)</p>

<p><label for="Tree Based Method" class="margin-toggle">⊕</label>
<input type="checkbox" id="Tree Based Method" class="margin-toggle" />
<span class="marginnote"><img class="fullwidth" src="/ISPeL-content-machine-learning/machine-learning/5_Tree_Based_Method_Part1/tree4/dtree2.png" alt="" /><br /></span></p>

<p>&lt;/span&gt;</p>

<p>\(H(S_{sunny})=(\frac{3}{5})log_2(\frac{3}{5})-(\frac{2}{5})log_2(\frac{2}{5}) = 0.96\)
\(IG(S_{sunny},Humidity) = 0.96\)
\(IG(S_{sunny},Temperature) = 0.57\)
\(IG(S_{sunny},Wind) = 0.019\)</p>

<ul>
  <li>Final decision tree</li>
</ul>

<p><img src="dtree3.png" alt="" /></p>

<h2 id="lets-build-a-decision-tree">Let’s build a decision tree</h2>
<ul>
  <li>C4.5 algorithm advantages:
    <ul>
      <li>Inexpensive to construct</li>
      <li>Extremely fast at classifying unknown records</li>
      <li>Easy to interpret for small-sized trees</li>
      <li>Robust to noise (especially when methods to avoid overfitting are employed)</li>
      <li>Can easily handle redundant or irrelevant attributes (unless the attributes are interacting)</li>
    </ul>
  </li>
  <li>C4.5 algorithm disadvantages:
    <ul>
      <li>Space of possible decision trees is exponentially large. </li>
      <li>Greedy approaches are often <font color="red">unable</font> to find the best tree.</li>
      <li>Does not take into account interactions between attributes. For example, age and height</li>
      <li>Each decision boundary involves only a single attribute. This may not work for some complex situations.</li>
    </ul>
  </li>
</ul>

<h2 id="decision-tree-sample-code">Decision Tree: Sample Code</h2>
<ul>
  <li>Let’s have a look at: tree_based_method</li>
  <li>We can control overfitting issue with max_depth, max_leaf_nodes</li>
</ul>

<table>
  <tbody>
    <tr>
      <td> </td>
      <td><a href="../../../">Index</a></td>
      <td> </td>
      <td> </td>
      <td><a href="../">Prev</a></td>
      <td> </td>
      <td> </td>
      <td><a href="../tree4">Next</a></td>
      <td> </td>
      <td> </td>
    </tr>
  </tbody>
</table>



    </article>
    <footer>
<hr class="slender">
<div class="credits">
This work is supported by the <a href="https://www.nsf.gov/" target="_blank">NSF</a> IUSE/PFE:RED award No. <a href="https://www.nsf.gov/awardsearch/showAward?AWD_ID=1730568" target="_blank">1730568</a>. Site created with <a href="//jekyllrb.com" target="_blank">Jekyll</a> using the <a href="//github.com/clayh53/tufte-jekyll" target="_blank">Tufte theme</a>. &copy; 2020
</div>
</footer>
  </body>
</html>
